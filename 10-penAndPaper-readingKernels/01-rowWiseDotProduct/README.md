# Row-wise Dot product
Code with some inefficiencies generated by ChatGPT for me to try and evaluate

## Reading a new Kernel?
Remembering my checklist as below:
1. Count Bytes:
    - Read/Writes from global memory
    - Read/Writes from shared memory
2. Think about problem algorithmically
    - What bottleneck SHOULD it be?
    - Don't optimise the wrong one lol
3. Count FLOPs:
4. Memory access and Operation Ordering: 
    1. Coalescence (are warp reads contiguous, yes or no?)
    2. Segments
        - 95% of the time, global memory loads are 128B = 32x4
        - Do my loads execute in 1 or more transactions?
    3. Tail
        - Small problem size = how many wasted bytes
    4. Instruction Level Parallelism
        - Instruction chaining e.g. num accumulators
    5. Parallelism: Enough blocks/warps overall to cover estimated load times?

## Kernel Details:
- Row-wise dot product
- 1 dimensional block
- A and B are row-major
- Call blockDim.x == block size == BX

### Initial blockwise accumulation:
```cpp
int row = blockIdx.x;
int tx  = threadIdx.x;
if (row >= M) return;

float acc = 0.0f;
int base = row * N;

// Each thread walks the row with a stride of blockDim.x
for (int j = tx; j < N; j += blockDim.x) {
    float a = A[base + j];
    float b = B[base + j];
    acc += a * b;
}
```

#### 1) Counting bytes
- Global reads:
  - `2N` One for each element of A and B 
    - `2N*4 bytes`
- Global writes:
  - `0`
- Shmem reads:
  - `0`
- Shmem writes:
  - `0`

#### 2) What should the bottleneck be here
There is 1 FLOP in this loop.
- The bottleneck should be **memory**

#### 3) FLOPs
- Addresses are integer ops
- The only FLOP is `a*b`
- `N` flops in this loop
- CORRECTION
  - There are of course TWO operations in `+=`, oops
  - `2N` Flops here
- Arithmetic intensity is NOW `2FLOPs/8 bytes = 0.25 FLOP/byte`
  - This seems very low

#### 4) Memory access and Operation Ordering
##### i) Coalescence
- Here the addresses for each read of A are mutually coalesced:
  - A and B are float*
  - Indexes across warp differ by only the thread index

##### ii) Segments
- I think segments here depend on if we are lucky with N
    - CORRECTION: We'll be happy with N is `row*N` is a multiple of 32, which it will be for a nice block size
- ~~The first block has 32 floats worth of reading starting from 0~~
  - `-> 32*4 = 128 bytes contiguous`
  - The warps in the first block will be ready in only 1 transaction
- The rest of the warps are likely to be by 2 transactions simply because of poor alignment dictated by N

CORRECTION:
- Remember that is `row*N` is a multiple of 32 we're fine

##### iii) Tail
- If N is tiny, we are not wasting much. Much of the slowdown is unavoidable
- CORRECTION: I apparently understated this. There would be smart ways to deal with this. i.e. by having warps handle individual rows

##### iv) ILP
- There are 2 loads before the dependency chain
- We can likely afford more registers with more accumulators
- ~~I wonder if we should be using tiled shared memory for this, or simply maximising registers?~~ No
- Either way, this is not the primary bottleneck

##### v) Occupancy & Parallelism
- This should have no problem hitting top occupancy alone
- CORRECTION:
  - Too vague:
    - Per-block occupancy is likely fine
    - The whole "one block per row" means that if N is small, we'll be using a subset of the SMs, which is very wasteful